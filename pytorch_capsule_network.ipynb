{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUA3Xio2/YftMtbTC6xg5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ugomezjr/Capsule-Networks-Pytorch/blob/main/pytorch_capsule_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_OLAuj5d831Q"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_digit = torch.randn(1, 28, 28)"
      ],
      "metadata": {
        "id": "EBfo2cCAm3ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer 1: ReLU Conv1\n",
        "\n",
        "Conv1 has 256 feature maps, 9 x 9 convolutional kernels with a stride of 1 and ReLU activation. \n",
        "\n",
        "* Converts pixel intensities to the activities of local feature detectors that are then used as inputs to the *primary* capsules. "
      ],
      "metadata": {
        "id": "va-oClR-mCDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This layer converts pixel intensities to the activities of local feature\n",
        "# detectors that are then used as inputs to the \"primary\" capsules. \n",
        "relu_conv1_block = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1,\n",
        "              out_channels=256,\n",
        "              kernel_size=9,\n",
        "              stride=1,\n",
        "              padding=0),\n",
        "    nn.ReLU()\n",
        ")"
      ],
      "metadata": {
        "id": "GKwITIK29RZ5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu_conv1_block(img_digit.unsqueeze(0)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjmeK7gb-Dzb",
        "outputId": "969dd65e-5b50-41f4-ad04-3a75ae8344f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 20, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer 2: PrimaryCapsules\n",
        "\n",
        "A convolutional capsule layer with 32 channels of convolutional 8D capsules (i.e. each primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride of 2).\n",
        "\n",
        "*   Lowest level of multi-dimensional entities.\n",
        "*   Activating the primary capsules corresponds to inverting the rendering process. \n",
        "\n"
      ],
      "metadata": {
        "id": "ugEp2Rx1i9T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In total PrimaryCapsules has [32 x 6 x 6] capsule outputs (each output is an 8D vector) and\n",
        "# each capsule in the [6 x 6] grid is sharing their weights with each other. \n",
        "\n",
        "class PrimaryCaps(nn.Module):\n",
        "  def __init__(self, \n",
        "               in_channels: int=256, \n",
        "               out_channels: int=32,\n",
        "               caps_dim: int=8, \n",
        "               kernel_size: int=9,\n",
        "               stride: int=2,\n",
        "               padding: int=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.caps_dim = caps_dim\n",
        "\n",
        "    self.conv2d = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels,\n",
        "                  out_channels=(out_channels*caps_dim),\n",
        "                  kernel_size=kernel_size,\n",
        "                  stride=stride,\n",
        "                  padding=padding)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    print(f\"Input Shape [ReLU Conv1]: {x.shape}\")\n",
        "    batch_size = x.size(0)\n",
        "    x = self.conv2d(x)\n",
        "    print(f\"PrimaryCaps Shape [original]: {x.shape}\")\n",
        "    x = x.view(batch_size, -1, self.caps_dim)\n",
        "    print(f\"PrimaryCaps Shape [reshaped]: {x.shape}\")\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "721a8aEiBosO"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primarycaps = PrimaryCaps()\n",
        "output = primarycaps(torch.randn(1, 256, 20, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWOEgR5EiUyS",
        "outputId": "253cb6b1-0f55-4a2f-c9ed-647fd9198cd5"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape [ReLU Conv1]: torch.Size([1, 256, 20, 20])\n",
            "PrimaryCaps Shape [original]: torch.Size([1, 256, 6, 6])\n",
            "PrimaryCaps Shape [reshaped]: torch.Size([1, 1152, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 1: Non-Linear \"**Squashing**\" Function"
      ],
      "metadata": {
        "id": "SBf4doqLnKGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The length of the vector output of a capsule cannot exceed 1 by applying a non-linearity that leaves\n",
        "# the orientation of the vector unchanged but scales down its magnitude.\n",
        "def squash(sj: torch.Tensor):\n",
        "  norm = torch.linalg.norm(sj)\n",
        "  norm_sqr = norm**2\n",
        "\n",
        "  return (norm_sqr / (1 + norm_sqr)) * (sj / norm)"
      ],
      "metadata": {
        "id": "JQH74Syltvzh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x: torch.Tensor):\n",
        "  sum_of_sqrs = torch.sum(torch.Tensor([ i**2 for i in x]))\n",
        "  return torch.sqrt(sum_of_sqrs)\n",
        "\n",
        "x1 = torch.arange(9, dtype=torch.float)\n",
        "x2 = torch.zeros(9, dtype=torch.float)\n",
        "\n",
        "print(torch.norm(x1), norm(x1))\n",
        "print(torch.norm(x2), norm(x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ub_vzRugyL",
        "outputId": "05eea5ce-80f9-4b86-f588-798103ae1c4e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14.2829) tensor(14.2829)\n",
            "tensor(0.) tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "class CapsNet(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               in_channels: int=1,\n",
        "               conv1_out_channels: int=256,\n",
        "               primary_caps_out_channels: int=32, \n",
        "               caps_dim: int=8,\n",
        "               kernel_size: int=9):\n",
        "    super().__init__()\n",
        "\n",
        "    self.caps_dim = caps_dim\n",
        "\n",
        "    self.relu_conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1,\n",
        "                  out_channels=256,\n",
        "                  kernel_size=9,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.primary_caps = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=256,\n",
        "                  out_channels=32*8,\n",
        "                  kernel_size=9,\n",
        "                  stride=2,\n",
        "                  padding=0)\n",
        "    )\n",
        "  \n",
        "\n",
        "  def squash(self, sj: torch.Tensor, dim: int=-1):\n",
        "    \"\"\"\n",
        "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
        "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
        "    return scale * inputs\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # Original Implementation:\n",
        "\n",
        "    norm = torch.linalg.norm(sj)\n",
        "    norm_sqr = norm**2\n",
        "\n",
        "    return (norm_sqr / (1 + norm_sqr)) * (sj / norm)\n",
        "    \"\"\"\n",
        "\n",
        "    norm = torch.linalg.norm(sj, ord=2, dim=dim, keepdim=True)\n",
        "    norm_sqr = norm**2\n",
        "\n",
        "    scale = norm_sqr / (1 + norm_sqr) / (norm + 1e-8)\n",
        "    return scale * sj\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    x = self.relu_conv1(x)\n",
        "    print(f\"ReLU Conv1 Shape: {x.shape}\")\n",
        "    x = self.primary_caps(x)\n",
        "    print(f\"PrimaryCaps Shape [original]: {x.shape}\")\n",
        "    x = x.view(batch_size, -1, self.caps_dim) # x.reshape(x_size, -1, 8)\n",
        "    print(f\"PrimaryCaps Shape [reshaped]: {x.shape}\")\n",
        "    x = self.squash(x)\n",
        "    print(f\"PrimaryCaps shape [squashed]: {x.shape}\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "7HcBo5Q_BOZ5"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CapsNet()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wfZbP09C5mH",
        "outputId": "f09ab947-f474-4a5d-fada-8375916ba635"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CapsNet(\n",
              "  (relu_conv1): Sequential(\n",
              "    (0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (primary_caps): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(img_digit.unsqueeze(0))\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLoTxc-mC8HA",
        "outputId": "90eae328-5441-4cfb-8af1-dfa4d1912e2d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Conv1 Shape: torch.Size([1, 256, 20, 20])\n",
            "PrimaryCaps Shape [original]: torch.Size([1, 256, 6, 6])\n",
            "PrimaryCaps Shape [reshaped]: torch.Size([1, 1152, 8])\n",
            "PrimaryCaps shape [squashed]: torch.Size([1, 1152, 8])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1852,  0.1201,  0.1423,  ...,  0.3105,  0.1277,  0.3101],\n",
              "         [ 0.1490,  0.3131,  0.1541,  ...,  0.1951,  0.0989,  0.1846],\n",
              "         [ 0.3402,  0.2957,  0.1792,  ...,  0.0869,  0.2138,  0.0988],\n",
              "         ...,\n",
              "         [-0.0140,  0.0277,  0.0024,  ..., -0.0039, -0.0209, -0.0052],\n",
              "         [-0.0664, -0.0437, -0.1837,  ...,  0.0165,  0.0360, -0.1984],\n",
              "         [-0.0148, -0.0383, -0.0140,  ..., -0.0247,  0.0270, -0.0262]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(outputs[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTORD-vG-R23",
        "outputId": "4a61abec-3565-4aa4-d433-8abe5b273cc6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5287, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "32*6*6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIYDPfPuJdLn",
        "outputId": "780442e5-511c-4ca1-f4f3-73985f60ff94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1152"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "32*8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yef58NVDJ0yi",
        "outputId": "59d8da6c-c1d6-48a2-e078-b93256ca5bab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}